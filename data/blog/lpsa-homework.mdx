---
title: Pancreatic cancer statistical analysis
date: '2022-05-09'
tags: []
draft: false
---

_Excerpt from a notebook produced for a homework of the subject Statistical Learning for the Master's degree course at the University of Trento, academic year 2021/22. It may contain errors as it was susceptible to a final evaluation._

# Importing the CSV

The CSV containing the dataset is imported.

    prostate <- read.csv("prostate.csv")  # read csv file

# Data exploration

A dataset containing a number of clinical measures taken in 97 men who
were about to receive radical prostatectomy is given.

Before investigating in depth the correlations between the response
variable _lpsa_ and all the other explanatory variables that were
measured, it is good to investigate the structure of the dataset and the
variables themselves.

## Basic descriptive statistics

    names(prostate)

    ## [1] "lcavol"  "lweight" "age"     "lbph"    "svi"     "lcp"     "gleason"
    ## [8] "pgg45"   "lpsa"

The variables are:

- _lpsa_: level of prostate-specific antigen, in ng/ml and log scaled
- _lcavol_: log(cancer volume in cm3)
- _lweight_: log(prostate weight in g)
- _age_ in years
- _lbph_: log(amount of benign prostatic hyperplasia in cm2)
- _svi_: seminal vesicle invasion (1 = yes, 0 = no)
- _lcp_: log(capsular penetration in cm)
- _gleason_: Gleason score for prostate cancer (6,7,8,9)
- _pgg45_: percentage of Gleason scores 4 or 5, recorded over their
  visit history before their final current Gleason score

From [WebMD](https://www.webmd.com/prostate-cancer/guide/psa),
_Prostate-Specific Antigen (PSA) is something made by the prostate
gland._ High PSA levels may be a sign of prostate cancer, a noncancerous
condition such as prostatitis, or an enlarged prostate gland.

Given that all of the men observed had undergone prostatectomy, it will
be interesting to understand whether high PSA levels correspond to
larger measures of cancer, or whether there is a correlation with the
Gleason score. This is what is expected at a very superficial level of
analysis.

## Variable types

A check is made on the type of variables previously mentioned.

    str(prostate)

    ## 'data.frame':    97 obs. of  9 variables:
    ##  $ lcavol : num  -0.58 -0.994 -0.511 -1.204 0.751 ...
    ##  $ lweight: num  2.77 3.32 2.69 3.28 3.43 ...
    ##  $ age    : int  50 58 74 58 62 50 64 58 47 63 ...
    ##  $ lbph   : num  -1.39 -1.39 -1.39 -1.39 -1.39 ...
    ##  $ svi    : int  0 0 0 0 0 0 0 0 0 0 ...
    ##  $ lcp    : num  -1.39 -1.39 -1.39 -1.39 -1.39 ...
    ##  $ gleason: int  6 6 7 6 6 6 6 6 6 6 ...
    ##  $ pgg45  : int  0 0 20 0 0 0 0 0 0 0 ...
    ##  $ lpsa   : num  -0.431 -0.163 -0.163 -0.163 0.372 ...

The measurements are all numerical and are distinguished between integer
and float. Thus, we will rely on statistical regression models.

## Checking missing values

Another check is carried out on the presence of missing values, on
which, if necessary, a pre-processing should be performed.

    colSums(is.na(prostate))

    ##  lcavol lweight     age    lbph     svi     lcp gleason   pgg45    lpsa
    ##       0       0       0       0       0       0       0       0       0

Since there are no missing values, the data does not need to be
pre-processed and can be used as is.

## Basic summary of dataset

A SPLOM is generated to better investigate the relationships between
variables and their entity.

    pairs.panels(prostate)

![](/lpsa-homework/unnamed-chunk-6-1.png)

An eye to the age:

    summary(prostate$age)

    ##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
    ##   41.00   60.00   65.00   63.87   68.00   79.00

# Training and test set

Data is divided into two halves for training and testing.

    set.seed(1)
    prostate_split <- initial_split(prostate, prop=0.5)
    x_train <- training(prostate_split)
    x_test <- testing(prostate_split)
    y_test <- x_test$lpsa

# Decision Tree

## Fit on the whole data

A Regression Tree is fitted:

    tree_prostate <- tree(lpsa ~ ., data=prostate)
    summary(tree_prostate)

    ##
    ## Regression tree:
    ## tree(formula = lpsa ~ ., data = prostate)
    ## Variables actually used in tree construction:
    ## [1] "lcavol"  "lweight" "pgg45"
    ## Number of terminal nodes:  9
    ## Residual mean deviance:  0.4119 = 36.24 / 88
    ## Distribution of residuals:
    ##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
    ## -1.499000 -0.488000  0.003621  0.000000  0.481200  1.380000

It is possible to graphically visualize the development of the Decision
Tree.

    plot(tree_prostate)
    text(tree_prostate, pretty=0)
    title(main="Prostate: Unpruned regression tree")

![](/lpsa-homework/unnamed-chunk-10-1.png)

## Choosing tree complexity by CV

A Cross-Validation is performed to choose the complexity of the tree and
to decide, based on the results, if the tree will need to be pruned.

    set.seed(1)
    cv.prostate <- cv.tree(object=tree_prostate)
    names(cv.prostate)

    ## [1] "size"   "dev"    "k"      "method"

    cv.prostate

    ## $size
    ## [1] 9 8 7 6 5 4 3 2 1
    ##
    ## $dev
    ## [1]  88.24714  85.80665  84.14467  84.14467  80.89206  80.87758  82.89215
    ## [8] 111.76913 140.06860
    ##
    ## $k
    ## [1]      -Inf  1.687786  2.746385  2.758375  4.427103  4.445951  7.587544
    ## [8] 23.619667 44.401279
    ##
    ## $method
    ## [1] "deviance"
    ##
    ## attr(,"class")
    ## [1] "prune"         "tree.sequence"

We see that the tree with `cv.carseats$size[which.min(cv.carseats$dev)]`
terminal nodes has the lowest CV error rate of `min(cv.carseats$dev)`.
We can visualize the CV error as a function of either `size` and `k`:

    op <- par(mfrow=c(1, 2))
    plot(cv.prostate$size, cv.prostate$dev, type="b")
    plot(cv.prostate$k, cv.prostate$dev, type="b")

![](/lpsa-homework/unnamed-chunk-12-1.png)

    par(op)

Let’s found the optimal size:

    opt.size <- cv.prostate$size[which.min(cv.prostate$dev)]

    opt.size

    ## [1] 4

Now that the optimal number of terminal nodes is obtained, it is
possible to prune the tree:

    prune_prostate <- prune.tree(tree_prostate,
                               best=4)
    plot(prune_prostate)
    text(prune_prostate, pretty=0)
    title(main="Prostate: Pruned regression tree")

![](/lpsa-homework/unnamed-chunk-14-1.png)
The graphical view already suggests the development of the tree and the
variables that have been used, but it is still possible to produce a
descriptive summary to have a more accurate level of detail.

    summary(prune_prostate)

    ##
    ## Regression tree:
    ## snip.tree(tree = tree_prostate, nodes = c(11L, 3L, 10L))
    ## Variables actually used in tree construction:
    ## [1] "lcavol"  "lweight"
    ## Number of terminal nodes:  4
    ## Residual mean deviance:  0.5625 = 52.31 / 93
    ## Distribution of residuals:
    ##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
    ## -1.66200 -0.54020 -0.01154  0.00000  0.44560  1.81700

# Random Forests

The second statistical model we will use is Random Forests.

First of all, the variable _nvar_ will be defined, corresponding to the
number of explanatory variables.

The goal will be to find the optimal number of explanatory variables to
use for the model, corresponding to the variable _mtry_.

A K-Fold Cross Validation Schema will be defined:

1.  Initially, through Caret’s “train” method, it will be checked which
    variable is used for tuning the _mtry_ parameter;
2.  Next, another extension of the Cross Validation will be made,
    specifying the range from 1 to _nvar_ as the input variable.

A matrix will then be generated, to make a comparison between the CV
errors for each choice of _mtry_ and the OOB errors.

From here, some conclusions will be drawn.

## Fitting Random Forests

Application of random forests.

    set.seed(1)

    # setting nvar equal to the number of explanatory variables
    nvar <- ncol(prostate) - 1

    cv.error <- rep(0, nvar)

    # defining the K-Fold CV
    trControl <- trainControl(method = "cv", number = 10, search= "grid")

    # defining the Random Forest model with K-Fold CV
    rf_default <- train(lpsa ~ .,
                        data = prostate,
                        method = "rf",
                        metric = "RMSE",
                        trControl = trControl)

    # results
    rf_default

    ## Random Forest
    ##
    ## 97 samples
    ##  8 predictor
    ##
    ## No pre-processing
    ## Resampling: Cross-Validated (10 fold)
    ## Summary of sample sizes: 86, 89, 88, 86, 88, 89, ...
    ## Resampling results across tuning parameters:
    ##
    ##   mtry  RMSE       Rsquared   MAE
    ##   2     0.7644273  0.5674918  0.6114856
    ##   5     0.7392614  0.6038524  0.5944153
    ##   8     0.7465334  0.6044691  0.6001525
    ##
    ## RMSE was used to select the optimal model using the smallest value.
    ## The final value used for the model was mtry = 5.

The final value used for the model was mtry = 5, with a RMSE of 0.739.

Let’s search the best _mtry_, testing the model with values of mtry from
1 to nvar.

    set.seed(1)

    tuneGrid <- expand.grid(.mtry = c(1:nvar))

    rf_mtry <- train(lpsa ~ .,
                     data = prostate,
                     method = "rf",
                     metric= "RMSE",
                     tuneGrid = tuneGrid,
                     trControl = trControl,
                     importance = TRUE)

    rf_mtry

    ## Random Forest
    ##
    ## 97 samples
    ##  8 predictor
    ##
    ## No pre-processing
    ## Resampling: Cross-Validated (10 fold)
    ## Summary of sample sizes: 86, 89, 88, 86, 88, 89, ...
    ## Resampling results across tuning parameters:
    ##
    ##   mtry  RMSE       Rsquared   MAE
    ##   1     0.7922727  0.5462360  0.6269447
    ##   2     0.7582377  0.5721109  0.6075391
    ##   3     0.7454722  0.5888348  0.5969590
    ##   4     0.7373828  0.6087325  0.5946707
    ##   5     0.7458378  0.5992859  0.6010076
    ##   6     0.7383809  0.6111336  0.5952687
    ##   7     0.7473774  0.6070457  0.5993037
    ##   8     0.7529292  0.5994907  0.6054596
    ##
    ## RMSE was used to select the optimal model using the smallest value.
    ## The final value used for the model was mtry = 4.

Let’s do the final fit:

    set.seed(1)
    rf.prostate <- randomForest(lpsa ~ ., data=x_train, mtry=4, importance=TRUE)
    yhat.rf <- predict(rf.prostate, newdata=x_test)
    mean((yhat.rf - y_test)^2)

    ## [1] 0.5276155

## OOB errors vs RMSE CV

It is possible to compare CV errors and OOB errors.

    set.seed(1)

    results <- NULL

    # number of explanatory variables
    nvar <- ncol(prostate) - 1
    m <- 1:nvar

    results <- rbind(c("m/mtry", "OOB errors", "RMSE"))

    # OOB errors vs RMSE CV
    for (i in m) {
      rf_new <- randomForest(lpsa ~ ., mtry = i, data = prostate)
      # nodesize = inodesize
      results <- rbind(results, c(
                       i, mean((rf_new$predicted-prostate$lpsa[as.numeric(names(rf_new$predicted))])^2), rf_mtry$results$RMSE[i]))
    }

    results

    ##       [,1]     [,2]                [,3]
    ##  [1,] "m/mtry" "OOB errors"        "RMSE"
    ##  [2,] "1"      "0.673781312414739" "0.792272671272323"
    ##  [3,] "2"      "0.607484500400825" "0.758237739294216"
    ##  [4,] "3"      "0.603202889761718" "0.74547215206123"
    ##  [5,] "4"      "0.594253499609645" "0.737382779107833"
    ##  [6,] "5"      "0.595901760037468" "0.745837839778473"
    ##  [7,] "6"      "0.606357365414228" "0.738380932023391"
    ##  [8,] "7"      "0.621322796529856" "0.747377388257521"
    ##  [9,] "8"      "0.61854333523116"  "0.752929155236482"

It is possible to see how the OOB errors are different from the RMSE
values of the Cross Validation, however they reach the minimum at the
same value of _mtry_ and they are somehow aligned.

# Boosted Regression Tree

A Boosted Regression Tree will be fitted.

For the fitting, it will be asked to perform a tuning of the _n.trees_
and _interaction.depth_ parameters. Furthermore, a 10 K-Fold CV will be
processed.

    set.seed(1)

    tuneGrid <- expand.grid(
      n.trees = c(20, 50, 80, 110, 140, 170, 200, 500, 1000, 5000),
      interaction.depth = c(1,2,3,4),
      shrinkage = 0.1,
      n.minobsinnode = 10
    )

    ctrl <- trainControl(
      method = "cv",
      number = 10
    )

    boosted <- train(
      lpsa ~ .,
      data = prostate,
      method = 'gbm',
      preProcess = c("center", "scale"),
      trControl = ctrl,
      tuneGrid = tuneGrid,
      verbose = FALSE
    )

    #  gbm(lpsa ~ ., data=x_train, distribution="gaussian",
    #               n.trees=5000, interaction.depth=4)

It is checked what was done and what the results were.

    boosted

    ## Stochastic Gradient Boosting
    ##
    ## 97 samples
    ##  8 predictor
    ##
    ## Pre-processing: centered (8), scaled (8)
    ## Resampling: Cross-Validated (10 fold)
    ## Summary of sample sizes: 86, 89, 88, 86, 88, 89, ...
    ## Resampling results across tuning parameters:
    ##
    ##   interaction.depth  n.trees  RMSE       Rsquared   MAE
    ##   1                    20     0.8257158  0.5504330  0.6505063
    ##   1                    50     0.7988921  0.5618109  0.6233435
    ##   1                    80     0.8077620  0.5553525  0.6346772
    ##   1                   110     0.8171578  0.5459560  0.6401381
    ##   1                   140     0.8232322  0.5377540  0.6474675
    ##   1                   170     0.8281953  0.5348335  0.6488163
    ##   1                   200     0.8287056  0.5323609  0.6519578
    ##   1                   500     0.8607080  0.5023908  0.6665933
    ##   1                  1000     0.8981665  0.4587435  0.7215992
    ##   1                  5000     1.0760017  0.3384732  0.8566642
    ##   2                    20     0.7970120  0.5743682  0.6281374
    ##   2                    50     0.7934405  0.5680554  0.6232255
    ##   2                    80     0.7994193  0.5647133  0.6353379
    ##   2                   110     0.8050390  0.5601366  0.6375216
    ##   2                   140     0.8233413  0.5348279  0.6572957
    ##   2                   170     0.8384468  0.5240075  0.6677020
    ##   2                   200     0.8363277  0.5343328  0.6601492
    ##   2                   500     0.9047740  0.4572558  0.7060403
    ##   2                  1000     0.9626755  0.4119680  0.7537653
    ##   2                  5000     1.0627468  0.3348660  0.8514628
    ##   3                    20     0.7883959  0.5932070  0.6192349
    ##   3                    50     0.7904486  0.5750922  0.6183873
    ##   3                    80     0.8154337  0.5402537  0.6371361
    ##   3                   110     0.8333538  0.5162628  0.6469099
    ##   3                   140     0.8331505  0.5147492  0.6532153
    ##   3                   170     0.8485176  0.5111455  0.6696433
    ##   3                   200     0.8530700  0.5033560  0.6744010
    ##   3                   500     0.9202062  0.4425030  0.7341026
    ##   3                  1000     0.9886521  0.3827591  0.7997295
    ##   3                  5000     1.0627762  0.3292377  0.8647689
    ##   4                    20     0.7761779  0.5852444  0.6100175
    ##   4                    50     0.7896067  0.5583236  0.6229866
    ##   4                    80     0.8109744  0.5383160  0.6399322
    ##   4                   110     0.8284818  0.5297959  0.6622606
    ##   4                   140     0.8324957  0.5218478  0.6647095
    ##   4                   170     0.8365273  0.5218510  0.6641526
    ##   4                   200     0.8494545  0.5129361  0.6683855
    ##   4                   500     0.9131654  0.4566076  0.7360522
    ##   4                  1000     0.9854787  0.3913149  0.7953729
    ##   4                  5000     1.0489369  0.3455467  0.8516207
    ##
    ## Tuning parameter 'shrinkage' was held constant at a value of 0.1
    ##
    ## Tuning parameter 'n.minobsinnode' was held constant at a value of 10
    ## RMSE was used to select the optimal model using the smallest value.
    ## The final values used for the model were n.trees = 20, interaction.depth =
    ##  4, shrinkage = 0.1 and n.minobsinnode = 10.

The optimal model uses a value of 20 for _n.trees_ and an
_interaction.depth_ of 4. The _shrinkage_ and _n.minobsinnode_ variables
are kept constant. Indeed, these values correspond to the lowest RMSE.

# Comparing the performances of the three methods

Everything is ready to use the three optimal statistical models with the
training and testing datasets.

Let’s start with Regression Decision Trees:

    # tree fitting
    tree_prostate <- tree(lpsa ~ ., data=x_train)

    # cross validation
    cv.prostate <- cv.tree(object=tree_prostate)

    # optimal size
    opt.size <- cv.prostate$size[which.min(cv.prostate$dev)]

    opt.size

    ## [1] 4

_opt.size_ is used to make the final fit.

    prune_prostate <- prune.tree(tree_prostate,
                               best=opt.size)
    plot(prune_prostate)
    text(prune_prostate, pretty=0)
    title(main="Prostate: Pruned regression tree")

![](/lpsa-homework/unnamed-chunk-23-1.png)
Time to make predictions on the test set. For the scope, the pruned tree
is reused. Results are shown.

    yhat <- predict(prune_prostate,
                    newdata=x_test)
    plot(yhat, y_test)
    abline(0, 1)

![](/lpsa-homework/unnamed-chunk-24-1.png)

A check on the test set values of MSE and standard deviation is carried,
in order to be able to make further comparisons.

    mean((yhat - y_test)^2) # test set MSE

    ## [1] 0.7743445

    # sqrt(mean((yhat - y_test)^2)) #std dev

The same practice is repeated for the Random Forests model.

    # cross validation
    trControl <- trainControl(method = "cv", number = 10, search= "grid")

    # model
    rf_default <- train(lpsa ~ .,
                        data = x_train,
                        method = "rf",
                        metric = "RMSE",
                        trControl = trControl)

    yhat.rf <- predict(rf_default, newdata=x_test) # predictions
    mean((yhat.rf - y_test)^2) # MSE

    ## [1] 0.5631072

    # sqrt(mean((yhat - y_test)^2)) #std dev

Finally, the same is done for the Boosted Regression Tree.

    set.seed(1)

    # tuning parameters
    tuneGrid <- expand.grid(
      n.trees = 20,
      interaction.depth = 4,
      shrinkage = 0.1,
      n.minobsinnode = 10
    )

    # k-fold cv
    ctrl <- trainControl(
      method = "cv",
      number = 10
    )

    # model
    boosted <- train(
      lpsa ~ .,
      data = x_train,
      method = 'gbm',
      preProcess = c("center", "scale"),
      trControl = ctrl,
      tuneGrid = tuneGrid,
      verbose = FALSE
    )

    yhat <- predict(boosted, newdata=x_test, n.trees=20) # predictions
    mean((yhat - y_test)^2) # MSE

    ## [1] 0.6637914

    # sqrt(mean((yhat - y_test)^2)) #std dev

# Conclusions

Regarding data and their correlations, we can observe that:

- Apparently, the most important correlation is identified between
  _lpsa_ and _lcavol_;
- _svi_ is binary;
- another important correlation is detected between _gleason_ and
  _pgg45_.

_gleason_ (the Gleason Score) ranges from 1-10 and describes how much
the cancer from a biopsy looks like healthy tissue (lower score) or
abnormal tissue (higher score). _pgg45_ measures the percentage of
Gleason scores 4 or 5, recorded over their visit history before their
final current Gleason. The correlation would be justified by the
expectation that if the patient had a bad Gleason Score in the past,
then it is very likely that the final current Gleason will be just as
high, if not higher.

The minimum age is 41 and the maximum is 79. On average, 63. In any
case, it does not appear to be an impactful variable on PSA levels and
other explanatory variables.

During the Decision Tree fitting, it is possible to see how the model
took more consideration of the variables _lcavol_, _lweight_ and
_pgg45_.

In general, it is possible to assert that higher levels of _lcavol_ will
correspond to higher levels of _lpsa_. When the values of _lcavol_ are
low, a check is performed on the variables _lweight_ and _pgg45_: the
value of _lpsa_ is directly proportional to the clinical measures.

It is possible to identify how all the other clinical measures have no
relevant correlation with the response variable _lpsa_.

Regarding the comparison of models, here too some considerations can be
made. By comparing the MSE values of the three models, it can be stated
that the Random Forests model appears to be the most accurate. The
Decision Trees, on the other hand, are the less accurate ones.

This confirms our expectations. In fact, Decision Trees are simple
models to interpret but are based on a single branching strategy, while
the Random Forests model works with a set of decision trees.

Finally, I expected to find greater accuracy in the Boosted Regression
Trees, however at a first comparison at the level of MSE values it does
not seem to be the best choice, probably because the data set is too
small.
